{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e250133-19c8-43c9-b5e9-98f8b21f2066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-09-20 05:11:20.456163: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-09-20 05:11:20.484030: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-20 05:11:21.121079: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-09-20 05:11:21.121217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-09-20 05:11:21.121227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n",
      "wandb: Currently logged in as: seegong (aiffelthon). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anthonypark6904/CLR-module/wandb/run-20220920_051123-2vdz19ay</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aiffelthon/CLR/runs/2vdz19ay\" target=\"_blank\">giddy-flower-135</a></strong> to <a href=\"https://wandb.ai/aiffelthon/CLR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load song_meta.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 707989/707989 [00:00<00:00, 758564.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load complete!\n",
      "\n",
      "Load file list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 23.12it/s]\n",
      "wandb: Waiting for W&B process to finish... (success).                                                                                                                                                           \n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:  eval_loss █▂▁▁▁▁▁▁▁▁\n",
      "wandb: train_loss █▂▁▁▁▁▁▁▁▁\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:  eval_loss 0.00035\n",
      "wandb: train_loss 0.00043\n",
      "wandb: \n",
      "wandb: Synced giddy-flower-135: https://wandb.ai/aiffelthon/CLR/runs/2vdz19ay\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20220920_051123-2vdz19ay/logs\n",
      "https://symbolize.stripped_domain/r/?trace=7f7d40b9ac7f,7f7d40afb08f&map= \n",
      "*** SIGTERM received by PID 363478 (TID 363478) on cpu 89 from PID 356028; stack trace: ***\n",
      "PC: @     0x7f7d40b9ac7f  (unknown)  wait4\n",
      "    @     0x7f7af16e7294        976  (unknown)\n",
      "    @     0x7f7d40afb090  (unknown)  (unknown)\n",
      "    @ ... and at least 12 more frames\n",
      "https://symbolize.stripped_domain/r/?trace=7f7d40b9ac7f,7f7af16e7293,7f7d40afb08f&map=068bf80b76f830987166dd8847d0248f:7f7adc147000-7f7af1a79de0 \n",
      "E0920 05:53:32.850710  363478 coredump_hook.cc:324] RAW: Remote crash gathering disabled for SIGTERM.\n"
     ]
    }
   ],
   "source": [
    "from utils import augment\n",
    "from utils.augment import *\n",
    "\n",
    "from trainer import *\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from utils.dataloader import mel_dataset\n",
    "\n",
    "wandb.init(\n",
    "project='CLR',\n",
    "entity='aiffelthon'\n",
    ")\n",
    "\n",
    "data = mel_dataset('/mnt/disks/sdb/dev_dataset', 'total')\n",
    "\n",
    "mix = MixupBYOLA()\n",
    "crop = RandomResizeCrop()\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    x_train_1 = []\n",
    "    x_train_2 = []\n",
    "    \n",
    "    for x, y in batch:\n",
    "        x = (np.array(x)+127)/100\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "        x = crop(mix(x))        \n",
    "        x_train_1.append(x)\n",
    "        \n",
    "    for x, y in batch:\n",
    "        x = (np.array(x)+127)/100\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "        x = crop(mix(x))        \n",
    "        x_train_2.append(x)\n",
    "            \n",
    "    y_train = [y for _, y in batch]           \n",
    "    return augment.post_norm(np.stack(x_train_1 + x_train_2, axis=0)), np.array(y_train)\n",
    "\n",
    "def eval_collate_batch(batch):\n",
    "    x_train = [(np.array(x)+127)/100 for x, _ in batch]\n",
    "    y_train = [y for _, y in batch]                  \n",
    "        \n",
    "    return np.array(x_train), np.array(y_train)\n",
    "\n",
    "\n",
    "def train_simclr(num_epochs, **kwargs):\n",
    "    # Create a trainer module with specified hyperparameters\n",
    "    trainer = BYOL_ATrainer(exmp=jnp.ones((128,48,1876,1)))\n",
    "    trainer.train_model(train_dataloader, test_dataloader, num_epochs=num_epochs)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "        \n",
    "def prepare_data_features(encode_fn, data):\n",
    "    # Encode all images\n",
    "    dataset = DataLoader(data, batch_size=128,\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=True,\n",
    "                                  num_workers=0,\n",
    "                                  collate_fn=eval_collate_batch)\n",
    "    \n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(dataset):\n",
    "        batch_feats = encode_fn(batch_imgs)\n",
    "        feats.append(jax.device_get(batch_feats))\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = np.concatenate(feats, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    # Sort images by labels for easier postprocessing later\n",
    "    idxs = labels.argsort()\n",
    "    labels, feats = labels[idxs], feats[idxs]\n",
    "\n",
    "    return NumpyDataset(feats, labels)\n",
    "    \n",
    "    \n",
    "class NumpyDataset(mel_dataset):\n",
    "    # data.TensorDataset for numpy arrays\n",
    "\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [arr[idx] for arr in self.arrays]\n",
    "\n",
    "    \n",
    "def train_logreg(batch_size, train_feats_data, test_feats_data, num_epochs=100, **kwargs):\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_feats_data,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True,\n",
    "                                   generator=torch.Generator().manual_seed(42),\n",
    "                                   collate_fn=numpy_collate)\n",
    "    test_loader = DataLoader(test_feats_data,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  drop_last=False,\n",
    "                                  collate_fn=numpy_collate)\n",
    "\n",
    "    # Create a trainer module with specified hyperparameters\n",
    "    trainer = LGTrainer(exmp=next(iter(train_loader))[0],\n",
    "                        # model_suffix=model_suffix,\n",
    "                        **kwargs)\n",
    "    trainer.train_model(train_loader, test_loader, num_epochs=num_epochs)\n",
    "\n",
    "    # Test best model on train and validation set\n",
    "    train_result = trainer.eval_model(train_loader) \n",
    "    test_result = trainer.eval_model(test_loader)\n",
    "\n",
    "    return trainer    \n",
    "    \n",
    "dataset_size = len(data)\n",
    "train_size = int(dataset_size * 0.8)    \n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, test_dataset, = random_split(data, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=0, collate_fn=collate_batch)\n",
    "simclr_trainer = train_simclr(num_epochs=10)\n",
    "\n",
    "    \n",
    "#     simclr_model = simclr_trainer.model.bind({'params': simclr_trainer.state.params,\n",
    "#                                           'batch_stats': simclr_trainer.state.batch_stats},\n",
    "#                                         mutable=['batch_stats'])\n",
    "    \n",
    "#     encode_fn = jax.jit(lambda img: simclr_model.encode(img))\n",
    "#     train_feats_simclr = prepare_data_features(encode_fn, train_dataset)\n",
    "#     test_feats_simclr = prepare_data_features(encode_fn, test_dataset)\n",
    "    \n",
    "#     trainer = train_logreg(batch_size=128,\n",
    "#                          train_feats_data=train_feats_simclr,\n",
    "#                          test_feats_data=test_feats_simclr,\n",
    "#                          num_classes=30,\n",
    "#                          lr=1e-3,\n",
    "#                          weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8eefd-4650-41ff-aabe-2ece973af1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
